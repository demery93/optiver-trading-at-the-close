{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71902ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import groupby\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from itertools import combinations\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numba\n",
    "from numba import jit, njit, prange\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d7e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type the log background info\n",
    "experiment = 132\n",
    "experiment_description = \"exp131 + normalized reference_price\"\n",
    "logging.basicConfig(filename='../logs/experiment.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filemode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07bc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2066805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "# revealed_targets = pd.read_csv('../data/revealed_targets.csv')\n",
    "# test = pd.read_csv('../data/test.csv')\n",
    "# sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "train.dropna(subset=['target'], inplace=True)\n",
    "train['bucket'] = train['date_id']//97 # used for cv\n",
    "\n",
    "# try the stock info\n",
    "# stock_info = pd.read_csv('../data/sectornames_and_marketcap.csv')[['stock_id', 'sectorname']]\n",
    "\n",
    "# for col in ['sectorname']:\n",
    "#     le = LabelEncoder()\n",
    "#     stock_info[col] = le.fit_transform(stock_info[col])\n",
    "\n",
    "# train = train.merge(stock_info, on='stock_id', how='left')\n",
    "\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44758f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                         0\n",
       "date_id                          0\n",
       "seconds_in_bucket                0\n",
       "imbalance_size                 132\n",
       "imbalance_buy_sell_flag          0\n",
       "reference_price                132\n",
       "matched_size                   132\n",
       "far_price                  2894254\n",
       "near_price                 2857092\n",
       "bid_price                      132\n",
       "bid_size                         0\n",
       "ask_price                      132\n",
       "ask_size                         0\n",
       "wap                            132\n",
       "target                           0\n",
       "time_id                          0\n",
       "row_id                           0\n",
       "bucket                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee68267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_weights(df):\n",
    "    weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "    ]\n",
    "    match_stock = {}\n",
    "    for idx, w in enumerate(weights):\n",
    "        match_stock[idx] = w\n",
    "    \n",
    "    df['weight_type'] = df['stock_id'].map(match_stock)\n",
    "    return df\n",
    "\n",
    "# cutoff info on seconds_in_bucket\n",
    "def cut_off(seconds_in_bucket):\n",
    "    if seconds_in_bucket <= 300:\n",
    "        return 1\n",
    "    elif seconds_in_bucket <= 480:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "train = match_weights(train)\n",
    "train['cut_off_time'] = train['seconds_in_bucket'].apply(cut_off)\n",
    "\n",
    "stock_pca = pd.read_csv('../data/principal_components.csv')[['stock_id', 'PC1', 'PC2', 'PC3']]\n",
    "\n",
    "train = train.merge(stock_pca, on=['stock_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe63003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third step\n",
    "def global_features(df):\n",
    "    \"\"\"\n",
    "    TODO: size/median_sizes; useful features' ratio;\n",
    "    first_sizes and last_sizes looks wired -> can try to remove them; If they are useful, can consider to \n",
    "    use daily level to create the difference between them;\n",
    "    imbalance_momentum can be update to use multiple windows -> check the difference with the rolling features;\n",
    "    referene_prices over the middle of bid price and ask price;\n",
    "    If 5 mins features are useful, then we can try to replicate the useful features we have before, same to the cutoff;\n",
    "    Whether independent features are better than sum features -> test\n",
    "    \"\"\"\n",
    "    global_stock_id_feats = {\n",
    "        # size related features\n",
    "        \"median_size\": df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"max_sizes\": df.groupby('stock_id')['bid_size'].max() + df.groupby('stock_id')['ask_size'].max(),\n",
    "        \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"mean_sizes\": df.groupby('stock_id')['bid_size'].mean() + df.groupby('stock_id')['ask_size'].mean(),\n",
    "        \"ptp_IQR\": df.groupby(\"stock_id\")[\"bid_size\"].quantile(0.75) - df.groupby(\"stock_id\")[\"ask_size\"].quantile(0.25),\n",
    "        \n",
    "        # price related features\n",
    "        \"median_price\": df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        \n",
    "        # 5 mins behavior\n",
    "        \"last5min_medvol\": df[df.seconds_in_bucket>=300].groupby(\"stock_id\")['bid_size'].median() + df[df.seconds_in_bucket>=300].groupby(\"stock_id\")['ask_size'].median(),\n",
    "        \"first5min_medvol\": df[df.seconds_in_bucket<300].groupby(\"stock_id\")['bid_size'].median() + df[df.seconds_in_bucket<300].groupby(\"stock_id\")['ask_size'].median(),\n",
    "        \n",
    "        # cut-off related features\n",
    "        \"cutoff_mean_sizes\": df.groupby(['stock_id', 'cut_off_time'])['bid_size'].mean() + df.groupby(['stock_id', 'cut_off_time'])['ask_size'].mean(),\n",
    "        \"cutoff_imb_ratios\": df.groupby(['stock_id', 'cut_off_time'])['matched_size'].mean() / df.groupby(['stock_id', 'cut_off_time'])['imbalance_size'].mean(),\n",
    "    }\n",
    "\n",
    "    return global_stock_id_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e59c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e08387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step\n",
    "def feature_eng(df):\n",
    "    \"\"\"\n",
    "    TODO: price_pressure can apply with current imbalance_szie with spread_intensity -> this direction\n",
    "    can think deeper -> catching the imb info; \n",
    "    depth_pressure idea can be extended to (imbalance_size - matched_size) * (reference_price - mid_price);\n",
    "    matched_momentum like imbalance_momentum; \n",
    "    far_price-near_price;\n",
    "    windows size adjustment;\n",
    "    the windows part, can we add weight type group by info;\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60   \n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df['bid_ask_volume_diff'] = df.eval(\"ask_size - bid_size\")\n",
    "    df[\"liquidity_imbalance\"] = df['bid_ask_volume_diff']/df[\"volume\"]\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / (1+df['matched_size'])\n",
    "    \n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * df[\"price_spread\"]\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    #供需市场的差额\n",
    "    df['depth_pressure'] = df['bid_ask_volume_diff'] * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df[\"matched_momentum\"] = df.groupby(['stock_id'])['matched_size'].diff(periods=1) / df['imbalance_size']  \n",
    "    \n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"] \n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "    \n",
    "    df['near_ratio'] = df['near_price'] / df['reference_price']\n",
    "    df['far_ratio'] = df['far_price'] / df['reference_price']\n",
    "    df['near_size'] = df['near_ratio'] * df['matched_size']\n",
    "    df['far_size'] = df['far_ratio'] * df['matched_size']\n",
    "\n",
    "    # use the triplet method\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].pct_change(window)\n",
    "            df[f\"wt_{col}_ret_{window}\"] = df.groupby(['weight_type', 'date_id'])[col].pct_change(window)\n",
    "            \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].diff(window)\n",
    "    \n",
    "    # one hot encoding\n",
    "    df['cut_off_time_2'] = 0\n",
    "    df['cut_off_time_3'] = 0\n",
    "    df.loc[df[\"cut_off_time\"] == 2, \"cut_off_time_2\"] = 1\n",
    "    df.loc[df[\"cut_off_time\"] == 3, \"cut_off_time_3\"] = 1\n",
    "    # df.drop(columns=['cut_off_time'], inplace=True)\n",
    "    df['far_near_diff'] = df['far_price'] - df['near_price']\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df \n",
    "    \n",
    "#     # useful for the last fold\n",
    "#     df['all_prices_mean_std'] = df['all_prices_mean']/(1+df['all_prices_std'])\n",
    "#     df['all_sizes_mean_std'] = df['all_sizes_mean']/(1+df['all_sizes_std'])\n",
    "#     df['all_prices_skew_kurt'] = df['all_prices_skew']/(1+df['all_prices_kurt'])\n",
    "#     df['all_sizes_skew_kurt'] = df['all_sizes_skew']/(1+df['all_sizes_kurt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75433515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def std_group(value, mean, std):\n",
    "#     if value >= mean and value <= mean+0.5*std:\n",
    "#         return 1\n",
    "#     elif value >= mean-0.5*std and value < mean:\n",
    "#         return -1\n",
    "#     elif value > mean+0.5*std and value <= mean + std:\n",
    "#         return 2\n",
    "#     elif value < mean-0.5*std and value >= mean - std:\n",
    "#         return -2\n",
    "#     elif value > mean + std and value <= mean + 2*std:\n",
    "#         return 3\n",
    "#     elif value < mean - std and value >= mean - 2*std:\n",
    "#         return -3\n",
    "#     elif value > mean + 2*std and value <= mean + 3*std:\n",
    "#         return 4\n",
    "#     elif value < mean - 2*std and value >= mean - 3*std:\n",
    "#         return -4\n",
    "#     elif value > mean + 3*std:\n",
    "#         return 5\n",
    "#     else:\n",
    "#         return -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081e1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second step: very heavy calculation; should make the decision on keeping which part\n",
    "@jit(numba.float64[:, :](numba.float64[:, :]))\n",
    "def rolling_features(df):\n",
    "    \"\"\"\n",
    "    TODO: Use long window to replace the cum ops or training with cum ops but inference with using\n",
    "    the recorded value from the previous seconds in the same day;\n",
    "    With the considerations about using long window to repalce cum, maybe we can add windows options in\n",
    "    the rolling part and add std, skew, kurt ops here;\n",
    "    for the last 10 sec wap part, we can calculate the pct change ratio between wap and ind_wap;\n",
    "    ema part for near and far price;\n",
    "    should rolling all use min_periods=1 -> reduce the null ratio;\n",
    "    use rsi instead of rti;\n",
    "    rolling features use some features from first step;\n",
    "    buy_flag_diff can be optimized with calcualte the diff first then times the flag, \n",
    "    can also add skew and kurt into this part, some methods to calcuale the angle between two slope;\n",
    "    approx slope for wap, matched_size, imbalanced_size and so on;\n",
    "    can we avoid reset_index to save the memory;\n",
    "    1mins level wap and industries related features: add median and max, the difference between max and min\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ### Light Ops ###\n",
    "    # last 10 sec wap and the ratio between wap and the mean of previous wap in the same weight type\n",
    "    df['previous_wap'] = df.groupby(['stock_id', 'date_id'])['wap'].shift(1)\n",
    "    df['previous_wt_wap_mean'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])['previous_wap'].transform('mean')\n",
    "    df['wap_percent_ind'] = df['wap'] / df['previous_wt_wap_mean']\n",
    "    \n",
    "    # last 10 sec median match size and im size for each weight type and the ratio between im size and match size\n",
    "    df['previous_im_size'] = df.groupby(['stock_id', 'date_id'])['imbalance_size'].shift(1)\n",
    "    df['previous_m_size'] = df.groupby(['stock_id', 'date_id'])['matched_size'].shift(1)\n",
    "    df['previous_wt_im_median'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])['previous_im_size'].transform('median')\n",
    "    df['previous_wt_m_median'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])['previous_m_size'].transform('median')\n",
    "    df['previous_imbalance_ratio_percent_ind'] = df['imbalance_size'] / (1+df['matched_size'])/ df['previous_wt_im_median'] / (1+df['previous_wt_m_median'])\n",
    "    # drop at the end\n",
    "    # df.drop(columns=['previous_im_size', 'previous_m_size', 'previous_wt_im_median', 'previous_wt_m_median'], inplace=True)\n",
    "    \n",
    "    # ema\n",
    "    alpha = 0.285\n",
    "    beta = 1-alpha\n",
    "    for col in ['matched_size', 'wap', 'imbalance_size']:\n",
    "        df[f'ema_{col}'] = df[col]*alpha + df.groupby(['stock_id', 'date_id'])[col].shift(1)*alpha*beta + \\\n",
    "        df.groupby(['stock_id', 'date_id'])[col].shift(2)*alpha*beta**2 + df.groupby(['stock_id', 'date_id'])[col].shift(3)*alpha*beta**3 + \\\n",
    "        df.groupby(['stock_id', 'date_id'])[col].shift(4)*alpha*beta**4 + df.groupby(['stock_id', 'date_id'])[col].shift(5)*alpha*beta**5 + \\\n",
    "        df.groupby(['stock_id', 'date_id'])[col].shift(6)*alpha*beta**6\n",
    "        \n",
    "        for window in [3,5,7]:\n",
    "            df[f'rolling_{window}_{col}_mean'] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=window).mean().reset_index(level=[0, 1], drop=True)\n",
    "    \n",
    "    # get current index wap and previous 60 sec index wap and wap\n",
    "    # then get the ratio between current and previous 60 sec; also get the ratio between wap and iwap in current status and previous 60 sec\n",
    "    df['previous_1min_wap'] = df.groupby(['stock_id', 'date_id'])['wap'].shift(6)\n",
    "    df['1min_wap_ratio'] = df['wap'] / df['previous_1min_wap']\n",
    "    df['iwap'] = df['wap'] * df['weight_type']\n",
    "    iwap = df.groupby(['date_id', 'seconds_in_bucket']).agg(index_wap=('iwap', 'sum'), total_weight=('weight_type', 'sum')).reset_index()\n",
    "    iwap.sort_values(by=['date_id', 'seconds_in_bucket'], ascending=True, inplace=True)\n",
    "    iwap['previous_1min_iwap'] = iwap.groupby(['date_id'])['index_wap'].shift(6)\n",
    "    iwap['index_wap'] = iwap['index_wap']/iwap['total_weight']\n",
    "    iwap['previous_1min_iwap'] = iwap['previous_1min_iwap']/iwap['total_weight']\n",
    "    df = df.merge(iwap[['date_id', 'seconds_in_bucket', 'index_wap', 'previous_1min_iwap']], on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    df['1min_iwap_ratio'] = df['index_wap'] / df['previous_1min_iwap']\n",
    "    df['wap_index_ratio'] = df['wap']/df['index_wap']\n",
    "    df['1min_wap_index_ratio'] = df['previous_1min_wap']/df['previous_1min_iwap']\n",
    "    # drop at the end\n",
    "    # df.drop(columns=['index_wap'], inplace=True)\n",
    "    \n",
    "    # Replace 'df' with your DataFrame and 'time_id', 'wap', 'imbalance_buy_sell_flag' with your column names\n",
    "    df['norm_wap'] = df.groupby(\"time_id\")['wap'].transform('mean')\n",
    "    df['norm_wap'] = df['wap'] - df['norm_wap']\n",
    "    df['norm_imbalance_buy_sell_flag'] = df.groupby(\"time_id\")['imbalance_buy_sell_flag'].transform('mean')\n",
    "    df['norm_imbalance_buy_sell_flag'] = df['imbalance_buy_sell_flag'] - df['norm_imbalance_buy_sell_flag']\n",
    "\n",
    "    \n",
    "    # 浪起来写fe\n",
    "    for col in ['far_price', 'near_price',]:\n",
    "#     ['imbalance_size', 'reference_price', 'matched_size', 'bid_price', 'bid_size', \\\n",
    "#                'ask_price', 'ask_size', 'wap']: #'far_price', 'near_price', \n",
    "        df['previous_1_min'] = df.groupby(['stock_id', 'date_id'])[col].shift(6)\n",
    "        df[f'{col}_1min_approx_slope'] = df[col] - df['previous_1_min']\n",
    "        df[f'{col}_wt_1min_approx_slope_mean'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])[f'{col}_1min_approx_slope'].transform('mean')\n",
    "        df[f'{col}_wt_1min_approx_slope_std'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])[f'{col}_1min_approx_slope'].transform('std')\n",
    "        df[f'{col}_wt_1min_approx_slope_min'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])[f'{col}_1min_approx_slope'].transform('min')\n",
    "        df[f'{col}_wt_1min_approx_slope_max'] = df.groupby(['weight_type', 'date_id', 'seconds_in_bucket'])[f'{col}_1min_approx_slope'].transform('max')\n",
    "        df[f'{col}_1min_aslope_norm'] = (df[f'{col}_1min_approx_slope']-df[f'{col}_wt_1min_approx_slope_min'])/(1+df[f'{col}_wt_1min_approx_slope_max']-df[f'{col}_wt_1min_approx_slope_min'])\n",
    "        df[f'{col}_wt_1min_approx_slope_mean_std'] = df[f'{col}_wt_1min_approx_slope_mean']/df[f'{col}_wt_1min_approx_slope_std']\n",
    "    df['fp_np_aslope_ratio'] = df['far_price_1min_approx_slope']/df['near_price_1min_approx_slope']\n",
    "#     df['rp_ms_aslope_ratio'] = df['reference_price_1min_approx_slope']/df['matched_size_1min_approx_slope']\n",
    "#     df['rp_im_aslope_ratio'] = df['reference_price_1min_approx_slope']/df['imbalance_size_1min_approx_slope']\n",
    "#     df['im_ms_aslope_ratio'] = df['imbalance_size_1min_approx_slope']/df['matched_size_1min_approx_slope']\n",
    "#     df['ap_bp_aslope_ratio'] = df['ask_price_1min_approx_slope']/df['bid_price_1min_approx_slope']\n",
    "#     df['ap_bs_aslope_ratio'] = df['ask_price_1min_approx_slope']/df['bid_size_1min_approx_slope']\n",
    "#     df['bp_as_aslope_ratio'] = df['bid_price_1min_approx_slope']/df['ask_size_1min_approx_slope']\n",
    "#     df['fp_rp_aslope_ratio'] = df['far_price_1min_approx_slope']/df['reference_price_1min_approx_slope']\n",
    "#     df['np_rp_aslope_ratio'] = df['near_price_1min_approx_slope']/df['reference_price_1min_approx_slope']\n",
    "#     df['wap_im_ms_aslope_ratio'] = df['wap_1min_approx_slope']/(df['matched_size_1min_approx_slope']*df['imbalance_size_1min_approx_slope'])\n",
    "   \n",
    "    \n",
    "    ### Medium Ops ###\n",
    "    # rolling median with win=6 for wap, adjust im_size, reference_price\n",
    "    df['directed_imbalance_size'] = df['imbalance_size']*df['imbalance_buy_sell_flag']\n",
    "    for col in ['wap', 'directed_imbalance_size', 'reference_price']:\n",
    "        df[f'rolling_{col}_median'] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=6).median().reset_index(level=[0, 1], drop=True)\n",
    "        df[f'rolling_{col}_ratio'] = df[col] / df[f'rolling_{col}_median']\n",
    "    \n",
    "    # rolling sum in the 60 secs of (matched_size and im_size) & (bid_size, ask_size)\n",
    "    for (col1, col2) in [('matched_size', 'imbalance_size'), ('bid_size', 'ask_size')]:\n",
    "        df[f'roll_60_sum_{col1}']=df.groupby(['stock_id', 'date_id'])[col1].rolling(window=6).sum().reset_index(level=[0,1], drop=True)\n",
    "        df[f'roll_60_sum_{col2}']=df.groupby(['stock_id', 'date_id'])[col2].rolling(window=6).sum().reset_index(level=[0,1], drop=True)\n",
    "    # the ratio between match size and im size\n",
    "    df['roll_60_sum_ms_im_ratio'] = df['roll_60_sum_matched_size']/(1+df['roll_60_sum_imbalance_size'])*df['imbalance_buy_sell_flag']\n",
    "    # rolling 60 sec sum wap\n",
    "    df['roll_60_sum_wap'] = (df['roll_60_sum_ask_size'] * df['bid_price'] + df['roll_60_sum_bid_size'] * df['ask_price'])/(df['roll_60_sum_bid_size']+df['roll_60_sum_ask_size']) \n",
    "    \n",
    "    # approx slope for bid price, ask price, bid size and ask size\n",
    "    df['buy_flag'] = np.where(df['imbalance_buy_sell_flag'] >= 0, 1, np.nan)\n",
    "    df['sell_flag'] = np.where(df['imbalance_buy_sell_flag'] <= 0, 1, np.nan)\n",
    "    df['buy_time'] = df['buy_flag'] * df['seconds_in_bucket']\n",
    "    df['sell_time'] = df['sell_flag'] * df['seconds_in_bucket']\n",
    "    \n",
    "    for col in ['buy_time', 'sell_time']:\n",
    "        df[f'{col}_start'] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=6, min_periods=1).min().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{col}_end'] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=6, min_periods=1).max().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{col}_diff'] = df[f'{col}_end'] - df[f'{col}_start']\n",
    "    \n",
    "    for (buy, sell) in [('bid_price', 'ask_price'), ('bid_size', 'ask_size')]:\n",
    "        df['buy_site_flag'] = df['buy_flag'] * df[buy]\n",
    "        df['sell_site_flag'] = df['sell_flag'] * df[sell]\n",
    "        df['buy_flag_diff'] = df.groupby(['stock_id', 'date_id'])['buy_site_flag'].diff()\n",
    "        df['sell_flag_diff'] = df.groupby(['stock_id', 'date_id'])['sell_site_flag'].diff()\n",
    "        # calculate the sum of diff\n",
    "        df['buy_site_diff_sum'] = df.groupby(['stock_id', 'date_id'])[f'buy_flag_diff'].rolling(window=7, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{buy}_diff_rolling_std'] = df.groupby(['stock_id', 'date_id'])[f'buy_flag_diff'].rolling(window=7, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
    "        df['sell_site_diff_sum'] = df.groupby(['stock_id', 'date_id'])[f'sell_flag_diff'].rolling(window=7, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{sell}_diff_rolling_std'] = df.groupby(['stock_id', 'date_id'])[f'sell_flag_diff'].rolling(window=7, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
    "        # calculate the approx slope\n",
    "        df[f'{buy}_slope'] = df['buy_site_diff_sum']/ np.where(df['buy_time_diff'] == 0, np.nan, df['buy_time_diff'])\n",
    "        df[f'{sell}_slope'] = df['sell_site_diff_sum']/ np.where(df['sell_time_diff'] == 0, np.nan, df['sell_time_diff'])\n",
    "        # drop at the end\n",
    "        # df.drop(columns=['buy_site_flag', 'sell_site_flag','buy_flag_diff', 'sell_flag_diff', 'buy_site_diff_sum', 'sell_site_diff_sum'], inplace=True)\n",
    "    # drop at the end\n",
    "#     df.drop(columns=['buy_flag', 'sell_flag', 'buy_time', 'sell_time', 'buy_time_start', 'buy_time_end', \\\n",
    "#                      'sell_time_start', 'sell_time_end', 'buy_time_diff', 'sell_time_diff'], inplace=True)\n",
    "    \n",
    "    # rsi\n",
    "    for col in ['wap']:\n",
    "        df[f'{col}_diff'] = df.groupby(['stock_id', 'date_id'])[col].diff()\n",
    "        df[f'{col}_gain'] = df[f'{col}_diff'].where(df[f'{col}_diff'] > 0, 0)\n",
    "        df[f'{col}_loss'] = -df[f'{col}_diff'].where(df[f'{col}_diff'] < 0, 0)\n",
    "        for window in [7]:\n",
    "            df[f'{col}_avg_gain_{window}'] = df[f'{col}_gain'].rolling(window=window).mean()\n",
    "            df[f'{col}_avg_loss_{window}'] = df[f'{col}_loss'].rolling(window=window).mean()\n",
    "            df[f'{col}_rs_{window}'] = df[f'{col}_avg_gain_{window}']/df[f'{col}_avg_loss_{window}']\n",
    "            df[f'{col}_rsi_{window}'] = 100 - (100/(1+df[f'{col}_rs_{window}']))\n",
    "            \n",
    "            df.drop(columns=[f'{col}_avg_gain_{window}', f'{col}_avg_loss_{window}',\\\n",
    "                            f'{col}_rs_{window}'], inplace=True)\n",
    "        df.drop(columns=[f'{col}_diff', f'{col}_gain', f'{col}_loss',], inplace=True)\n",
    "            \n",
    "\n",
    "    # approx ratio for reference price diff\n",
    "    for col in ['reference_price']:\n",
    "        df[f'{col}_diff'] = df.groupby(['stock_id', 'date_id'])[col].diff()\n",
    "        df[f'{col}_diff_sum'] = df.groupby(['stock_id', 'date_id'])[f'{col}_diff'].rolling(window=7, min_periods=1).sum().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{col}_diff_rolling_std'] = df.groupby(['stock_id', 'date_id'])[f'{col}_diff_sum'].rolling(window=7, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
    "        df[f'{col}_diff_ratio'] = df[f'{col}_diff_sum']/np.where(df[f'{col}_diff'] == 0, np.nan, df[f'{col}_diff'])\n",
    "    \n",
    "    ### Heavy Ops ###\n",
    "    # adjusted rti\n",
    "    for col in ['matched_size', 'wap', 'reference_price']:\n",
    "        df[f'cum_{col}_std'] = df.groupby(['stock_id', 'date_id'])[col].expanding().std().reset_index(level=[0,1], drop=True)\n",
    "        df[f'upper_{col}'] = df[col] + df[f'cum_{col}_std']\n",
    "        df[f'lower_{col}'] = df[col] - df[f'cum_{col}_std']\n",
    "        df[f'rolling_upper_{col}_max'] = df.groupby(['stock_id', 'date_id'])[f'upper_{col}'].rolling(window=7).max().reset_index(level=[0,1], drop=True)\n",
    "        df[f'rolling_upper_{col}_min'] = df.groupby(['stock_id', 'date_id'])[f'lower_{col}'].rolling(window=7).min().reset_index(level=[0,1], drop=True)\n",
    "        df[f'rti_{col}'] = (df[col] - df[f'rolling_upper_{col}_min'])/(df[f'rolling_upper_{col}_max'] - df[f'rolling_upper_{col}_min'])\n",
    "        # drop at the end\n",
    "        # df.drop(columns=[f'upper_{col}', f'lower_{col}'], inplace=True)\n",
    "    # drop cum_wap_std; drop at the end\n",
    "    # df.drop(columns=['cum_wap_std'], inplace=True)   \n",
    "\n",
    "    # 1mins level wap and industries related features -> try to lower the difficulties on predicting the target\n",
    "    # todo: add median\n",
    "    df['previous_target'] = df['1min_wap_ratio'] - df['1min_iwap_ratio']\n",
    "    df[f'cum_previous_target_mean'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().mean().reset_index(level=[0,1], drop=True)\n",
    "    df[f'cum_previous_target_std'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().std().reset_index(level=[0,1], drop=True)\n",
    "    df[f'cum_previous_target_skew'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().skew().reset_index(level=[0, 1], drop=True)\n",
    "    df[f'cum_previous_target_kurtosis'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().kurt().reset_index(level=[0, 1], drop=True)\n",
    "    \n",
    "    # cumulative min, max and min max standard of the reference price, matched_size, imbalance_size\n",
    "    for col in ['reference_price', 'matched_size', 'imbalance_size']:\n",
    "        df[f'cum_{col}_min'] = df.groupby(['stock_id', 'date_id'])[col].expanding().min().reset_index(level=[0,1], drop=True)\n",
    "        df[f'cum_{col}_max'] = df.groupby(['stock_id', 'date_id'])[col].expanding().max().reset_index(level=[0,1], drop=True)\n",
    "        df[f'cum_{col}_max_min'] = (df[col] - df[f'cum_{col}_min'])/(1+df[f'cum_{col}_max'] - df[f'cum_{col}_min'])\n",
    "\n",
    "        \n",
    "    # drop at the end\n",
    "    # df.drop(columns=['cum_reference_price_min', 'cum_imbalance_size_min'], inplace=True)\n",
    "        \n",
    "    # pct_change for reference_price and wap and get the skew and kurt\n",
    "    for col in ['reference_price', 'wap',]:\n",
    "        df[f'{col}_pct_change'] = df.groupby(['stock_id', 'date_id'])[col].pct_change()\n",
    "        df[f'cum_{col}_skew'] = df.groupby(['stock_id', 'date_id'])[f'{col}_pct_change'].expanding().skew().reset_index(level=[0, 1], drop=True)\n",
    "        df[f'cum_{col}_kurtosis'] = df.groupby(['stock_id', 'date_id'])[f'{col}_pct_change'].expanding().kurt().reset_index(level=[0, 1], drop=True)\n",
    "        \n",
    "        # vol features\n",
    "        df[f'abs_return'] = df[f'{col}_pct_change'].abs()\n",
    "        df[f'{col}_illiq'] = (df['abs_return'] / df['matched_size']).rolling(10).mean()\n",
    "        \n",
    "        df['neg_retrun_flag'] = np.where(df[f'{col}_pct_change']<0, 1, 0)\n",
    "        df['neg_retrun_flag'] = df['neg_retrun_flag'] * df['abs_return']\n",
    "        df[f'{col}_negative_returns_illiq'] = df['neg_retrun_flag'].rolling(10).sum() / (1+(df['imbalance_size']/df['matched_size']).rolling(10).sum())\n",
    "        \n",
    "    # drop at the end\n",
    "    # df.drop(columns=['cum_wap_skew', 'reference_price_pct_change'], inplace=True)\n",
    "   \n",
    "    df.drop(columns=['previous_im_size', 'previous_m_size', 'previous_wt_im_median', 'previous_wt_m_median', \\\n",
    "                    'upper_matched_size', 'lower_matched_size', 'upper_wap', 'lower_wap', \\\n",
    "                    'upper_reference_price', 'lower_reference_price', 'buy_site_flag', 'sell_site_flag', \\\n",
    "                    'buy_flag_diff', 'sell_flag_diff', 'buy_site_diff_sum', 'sell_site_diff_sum', \\\n",
    "                    'buy_flag', 'sell_flag', 'buy_time', 'sell_time', 'buy_time_start', 'buy_time_end', \\\n",
    "                    'sell_time_start', 'sell_time_end', 'buy_time_diff', 'sell_time_diff', \\\n",
    "                    'cum_wap_skew', 'reference_price_diff', \\\n",
    "                    'cum_wap_std', 'cum_reference_price_min', 'cum_imbalance_size_min', 'index_wap', 'reference_price_pct_change',\\\n",
    "                    'abs_return', 'neg_retrun_flag', 'previous_1_min',\\\n",
    "                    ], inplace=True)\n",
    "\n",
    "    return df\n",
    "            \n",
    "        \n",
    "    \n",
    "#     # rsi\n",
    "#     df['gain'] = np.where(df['rp_diff']>0, df['rp_diff'], 0)\n",
    "#     df['loss'] = np.where(df['rp_diff']<0, -df['rp_diff'], 0)\n",
    "#     df['gain_flag'] = np.where(df['rp_diff']>0, 1, 0)\n",
    "#     df['loss_flag'] = np.where(df['rp_diff']<0, 1, 0)\n",
    "#     for window in [7,14]:\n",
    "#         df[f'gain_{window}_sum'] = df.groupby(['stock_id', 'date_id'])[f'gain'].rolling(window=window).sum().reset_index(level=[0,1], drop=True)\n",
    "#         df[f'loss_{window}_sum'] = df.groupby(['stock_id', 'date_id'])[f'loss'].rolling(window=window).sum().reset_index(level=[0,1], drop=True)\n",
    "#         df[f'gain_flag_{window}_sum'] = df.groupby(['stock_id', 'date_id'])[f'gain_flag'].rolling(window=window).sum().reset_index(level=[0,1], drop=True)\n",
    "#         df[f'loss_flag_{window}_sum'] = df.groupby(['stock_id', 'date_id'])[f'loss_flag'].rolling(window=window).sum().reset_index(level=[0,1], drop=True)\n",
    "#         df[f'gain_{window}_mean'] = df[f'gain_{window}_sum']/df[f'gain_flag_{window}_sum']\n",
    "#         df[f'loss_{window}_mean'] = df[f'loss_{window}_sum']/df[f'loss_flag_{window}_sum']\n",
    "#         df[f'relative_{window}_strength'] = df[f'gain_{window}_mean']/df[f'loss_{window}_mean']\n",
    "#         df[f'rsi__{window}'] = 1/(1+df[f'relative_{window}_strength'])\n",
    "#         'gain', 'loss', 'gain_flag', 'loss_flag', 'relative_7_strength', 'relative_14_strength', \\\n",
    "#                     'gain_7_sum', 'gain_flag_7_sum', 'gain_14_sum', 'gain_14_sum', \\\n",
    "#                     'loss_7_sum', 'loss_flag_7_sum', 'loss_14_sum', 'loss_flag_14_sum',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481330d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all features engineering func together\n",
    "def generate_all_features1(df):\n",
    "    df = feature_eng(df)\n",
    "    df = rolling_features(df)\n",
    "    return df\n",
    "\n",
    "def generate_all_features2(df):\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        if 'cut_off' not in key:\n",
    "            df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "        else:\n",
    "            df[f\"global_{key}\"] = df[\"stock_id\", 'cut_off_time'].map(value.to_dict())\n",
    "\n",
    "    # flag features\n",
    "    df['high_volume'] = np.where(df['volume'] > df['global_median_size'], 1, 0)\n",
    "    cols = [c for c in df.columns if c not in ['row_id', 'date_id','time_id', 'cut_off_time', 'stock_id']]\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4852f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    \n",
    "#    I got this idea from https://github.com/gotoConversion/goto_conversion/\n",
    "    \n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9bc2c72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 6.33187\n",
      "[200]\tvalid_0's l1: 6.32182\n",
      "[300]\tvalid_0's l1: 6.31985\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's l1: 6.31958\n",
      "Working on fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 7.00652\n",
      "[200]\tvalid_0's l1: 6.99073\n",
      "[300]\tvalid_0's l1: 6.98645\n",
      "[400]\tvalid_0's l1: 6.98488\n",
      "Early stopping, best iteration is:\n",
      "[420]\tvalid_0's l1: 6.98463\n",
      "Working on fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 5.69056\n",
      "[200]\tvalid_0's l1: 5.68069\n",
      "[300]\tvalid_0's l1: 5.67844\n",
      "[400]\tvalid_0's l1: 5.6775\n",
      "[500]\tvalid_0's l1: 5.67689\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's l1: 5.6767\n",
      "Working on fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 5.92044\n",
      "[200]\tvalid_0's l1: 5.91206\n",
      "[300]\tvalid_0's l1: 5.91075\n",
      "[400]\tvalid_0's l1: 5.91026\n",
      "Early stopping, best iteration is:\n",
      "[364]\tvalid_0's l1: 5.91009\n",
      "Working on fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 6.31201\n",
      "[200]\tvalid_0's l1: 6.30293\n",
      "[300]\tvalid_0's l1: 6.30211\n",
      "Early stopping, best iteration is:\n",
      "[348]\tvalid_0's l1: 6.30169\n",
      "CV mae: 6.238691221908206\n",
      "CV mae goto: 6.2391759844278\n",
      "change CV mae: 6.238839012243243\n",
      "the process run in 65.66213441292444 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lgb_params = {\n",
    "        \"objective\" : \"mae\",\n",
    "        \"n_estimators\" : 3000,\n",
    "        \"num_leaves\" : 128,\n",
    "        \"subsample\" : 0.6,\n",
    "        \"colsample_bytree\" : 0.6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"n_jobs\" : 4,\n",
    "        # \"device\" : \"gpu\",\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\" : \"gain\",\n",
    "    }\n",
    "\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "# model = lgb.LGBMRegressor(learning_rate=0.065, max_depth=6, n_estimators=800,\n",
    "#               num_leaves=32, objective='mae', random_state=13, metric_freq=50, \n",
    "#               reg_alpha=0.01, reg_lambda=3.25, colsample_bytree=0.7, subsample=0.65)\n",
    "\n",
    "# create rolling features which can be apply on the whole dataset -> important: online should be redefine this part of code\n",
    "train.sort_values(by=['stock_id', 'date_id', 'seconds_in_bucket'], ascending=True, inplace=True)\n",
    "\n",
    "cv_mae = 0\n",
    "cv_mae_goto = 0\n",
    "y_min, y_max = -64, 64\n",
    "change_cv_mae = 0\n",
    "\n",
    "cv_folds = [([0,1,4],2,3),([0,3,4],1,2),([2,3,4],0,1),([1,2,3],4,0), ([0,1,2],3,4)]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Working on fold {i+1}\")\n",
    "    train_bucket, val_bucket, test_bucket = cv_folds[i]\n",
    "    \n",
    "    test_ = train[train['bucket']==test_bucket]\n",
    "    train_val = train[train['bucket']!=test_bucket]\n",
    "    \n",
    "    # then the group features will be only calculated on train and val, not on test\n",
    "    train_val = generate_all_features1(train_val)\n",
    "    test_ = generate_all_features1(test_)\n",
    "    global_stock_id_feats = global_features(train_val)\n",
    "    train_val = generate_all_features2(train_val)\n",
    "    test_ = generate_all_features2(test_)\n",
    "    \n",
    "    train_val = reduce_mem_usage(train_val)\n",
    "    test_ = reduce_mem_usage(test_)\n",
    "    \n",
    "    train_ = train_val[train_val['bucket'].isin(train_bucket)]\n",
    "    val_ = train_val[train_val['bucket']==val_bucket]\n",
    "    \n",
    "    y_train, y_val, y_test = train_['target'], val_['target'], test_['target']\n",
    "    X_train, X_val, X_test = train_.drop(columns=['target', 'bucket',]), val_.drop(columns=['target', 'bucket',]), test_.drop(columns=['target', 'bucket',])\n",
    "    \n",
    "    eval_set = [(X_val, y_val)]\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, eval_metric='mae', callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=50),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    pred_goto = zero_sum(pred, X_test.loc[:,'bid_size'] + X_test.loc[:,'ask_size'])\n",
    "    \n",
    "    mae_pred = mean_absolute_error(y_test, pred)\n",
    "    mae_pred_goto = mean_absolute_error(y_test, pred_goto)\n",
    "    \n",
    "    cv_mae += mae_pred\n",
    "    cv_mae_goto += mae_pred_goto\n",
    "\n",
    "    # new added method for testing -> train more steps as we used to do\n",
    "#     infer_params = lgb_params.copy()\n",
    "#     infer_params[\"n_estimators\"] = int(1.2 * model.best_iteration_)\n",
    "#     infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n",
    "#     infer_lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "#     more_step_pred = infer_lgb_model.predict(X_test)\n",
    "#     more_step_pred_goto = zero_sum(more_step_pred, X_test.loc[:,'bid_size'] + X_test.loc[:,'ask_size'])\n",
    "    \n",
    "#     more_step_mae_pred = mean_absolute_error(y_test, more_step_pred)\n",
    "#     more_step_mae_pred_goto = mean_absolute_error(y_test, more_step_pred_goto)\n",
    "    \n",
    "#     more_step_cv_mae += more_step_mae_pred\n",
    "#     more_step_cv_mae_goto += more_step_mae_pred_goto\n",
    "    lgb_prediction = pred - np.mean(pred)\n",
    "    clipped_predictions = np.clip(lgb_prediction, y_min, y_max)\n",
    "    \n",
    "    change_mae_pred = mean_absolute_error(y_test, clipped_predictions)\n",
    "    change_cv_mae += change_mae_pred\n",
    "\n",
    "    \n",
    "print(f'CV mae: {cv_mae/5}')\n",
    "print(f'CV mae goto: {cv_mae_goto/5}')\n",
    "print(f'change CV mae: {change_cv_mae/5}')\n",
    "\n",
    "end = time.time()\n",
    "time_cost = (end - start)/60\n",
    "print(f'the process run in {time_cost} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65943857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 258\n"
     ]
    }
   ],
   "source": [
    "print(f'number of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc22ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Experiment {experiment}: {experiment_description}\")\n",
    "logging.info(f'Experiment {experiment} mae: {cv_mae/5}')\n",
    "logging.info(f'Experiment {experiment} mae_goto: {cv_mae_goto/5}')\n",
    "logging.info(f'Experiment {experiment} time series mae: {mae_pred}')\n",
    "logging.info(f'Experiment {experiment} change_mae: {change_cv_mae/5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a183c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = '暂时维持不适用ye transoform之后再看有没有统一好用的'\n",
    "logging.info(f\"Experiment {experiment} comment: {comments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd69cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
