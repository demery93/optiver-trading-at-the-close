{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303a1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import groupby\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from itertools import combinations\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numba\n",
    "from numba import jit, njit, prange\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa95738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3e5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "# revealed_targets = pd.read_csv('../data/revealed_targets.csv')\n",
    "# test = pd.read_csv('../data/test.csv')\n",
    "# sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "train.dropna(subset=['target'], inplace=True)\n",
    "train['bucket'] = train['date_id']//97 # used for cv\n",
    "\n",
    "# try the stock info\n",
    "# stock_info = pd.read_csv('../data/sectornames_and_marketcap.csv')[['stock_id', 'sectorname']]\n",
    "\n",
    "# for col in ['sectorname']:\n",
    "#     le = LabelEncoder()\n",
    "#     stock_info[col] = le.fit_transform(stock_info[col])\n",
    "\n",
    "# train = train.merge(stock_info, on='stock_id', how='left')\n",
    "\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4ca72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['seconds_in_bucket', 'imbalance_size', 'far_price', 'near_price', 'bid_price', 'ask_size', 'global_median_size', \n",
    "            'global_median_bid_size', 'global_median_ask_size', 'global_median_imbalance_size', 'global_median_matched_size', \n",
    "            'global_mean_imbalance_buy_sell_flag', 'global_std_size', 'global_ptp_size', 'global_median_price', 'global_std_price', \n",
    "            'global_std_wap', 'global_ptp_price', 'imbalance', 'imbalance_global_ratio', 'price_pressure', 'depth_pressure', \n",
    "            'reference_price_far_price_imb', 'reference_price_near_price_imb', 'reference_price_ask_price_imb', \n",
    "            'reference_price_bid_price_imb', 'reference_price_wap_imb', 'reference_price_mid_price_imb', 'far_price_near_price_imb', \n",
    "            'far_price_mid_price_imb', 'near_price_ask_price_imb', 'near_price_bid_price_imb', 'near_price_wap_imb', \n",
    "            'near_price_mid_price_imb', 'wap_mid_price_imb', 'matched_size_bid_size_imb', 'matched_size_ask_size_imb', \n",
    "            'matched_size_imbalance_size_imb', 'matched_size_mid_size_imb', 'bid_size_ask_size_imb', 'bid_size_imbalance_size_imb', \n",
    "            'ask_size_imbalance_size_imb', 'imbalance_size_mid_size_imb', 'imbalance_momentum', 'log_return', 'norm_wap', \n",
    "            'norm_imbalance_buy_sell_flag', 'norm_log_return', 'ask_price_bid_price_wap_imb2', 'ask_price_wap_reference_price_imb2', \n",
    "            'ask_price_reference_price_mid_price_imb2', 'bid_price_wap_reference_price_imb2', 'bid_price_reference_price_mid_price_imb2', \n",
    "            'matched_size_bid_size_ask_size_imb2', 'matched_size_bid_size_imbalance_size_imb2', 'matched_size_bid_size_mid_size_imb2', \n",
    "            'matched_size_ask_size_imbalance_size_imb2', 'matched_size_ask_size_mid_size_imb2', 'matched_size_imbalance_size_mid_size_imb2', \n",
    "            'bid_size_ask_size_imbalance_size_imb2', 'all_sizes_mean', 'all_prices_std', 'all_sizes_std', 'all_prices_skew', 'all_sizes_skew', \n",
    "            'all_prices_kurt', 'all_sizes_kurt', 'all_sizes_max', 'wap_rank', 'imbalance_buy_sell_flag_rank', 'seconds', 'minute', \n",
    "            'matched_size_first', 'matched_size_first_ratio', 'imbalance_size_first', 'imbalance_size_first_ratio', 'ask_size_first', \n",
    "            'ask_size_first_ratio', 'bid_size_first', 'imbalance_buy_sell_flag_cumsum', 'imbalance_buy_sell_flag_cummean', 'rsi_cumsum', \n",
    "            'rsi_cummean', 'wap_mid_price_imb_rank', 'imbalance_global_ratio_rank', 'matched_global_ratio_rank', 'ask_size_global_ratio_rank', \n",
    "            'market_imbalance_buy_sell_flag_rank', 'imbalance_buy_sell_flag_cumsum_rank', 'matched_size_ret_1', 'matched_size_ret_2', \n",
    "            'matched_size_ret_10', 'imbalance_size_ret_1', 'imbalance_size_ret_2', 'imbalance_size_ret_3', 'imbalance_size_ret_6', \n",
    "            'imbalance_size_ret_10', 'reference_price_ret_1', 'reference_price_ret_2', 'reference_price_ret_3', 'reference_price_ret_10', \n",
    "            'wap_ret_1', 'ask_price_ret_1', 'bid_price_ret_1', 'bid_price_ret_10', 'ask_size_ret_1', 'bid_size_ret_1', 'bid_size_ret_2', \n",
    "            'bid_size_ret_3', 'wap_rolling_std_3', 'wap_rolling_std_10', 'imbalance_buy_sell_flag_rolling_mean_3', \n",
    "            'imbalance_buy_sell_flag_rolling_std_3', 'imbalance_buy_sell_flag_rolling_mean_6', 'imbalance_buy_sell_flag_rolling_mean_10', \n",
    "            'imbalance_buy_sell_flag_rolling_std_10', 'imbalance_size_rolling_mean_3', 'imbalance_size_rolling_mean_10', \n",
    "            'matched_size_rolling_std_10', 'norm_wap_rolling_mean_6', 'norm_wap_rolling_mean_10', 'rsi_rolling_mean_3', 'rsi_rolling_mean_10', \n",
    "            'matched_size_ema', 'imbalance_size_ema', 'reference_price_wap_imb_shift_3', 'reference_price_wap_imb_shift_4', \n",
    "            'reference_price_wap_imb_shift_5', 'reference_price_wap_imb_shift_6', 'norm_wap_shift_1', 'norm_wap_shift_3', 'norm_wap_shift_5', \n",
    "            'imbalance_buy_sell_flag_shift_1', 'imbalance_buy_sell_flag_shift_2', 'imbalance_buy_sell_flag_shift_3', 'wap_rank_shift_1', \n",
    "            'wap_rank_shift_3', 'wap_rank_shift_4', 'wap_rank_shift_5', 'imbalance_buy_sell_flag_rank_shift_1', \n",
    "            'imbalance_buy_sell_flag_rank_shift_2', 'imbalance_buy_sell_flag_rank_shift_4', 'imbalance_buy_sell_flag_rank_shift_6', \n",
    "            'norm_log_return_shift_1', 'norm_log_return_shift_2', 'norm_log_return_shift_3', 'norm_log_return_shift_4', \n",
    "            'norm_log_return_shift_5', 'norm_log_return_shift_6', 'shifted_1_imbalance_buy_sell_flag', \n",
    "            'shifted_1_imbalance_buy_sell_flag_rank', 'shifted_1_target', 'shifted_2_imbalance_buy_sell_flag_rank', \n",
    "            'shifted_2_reference_price_wap_imb', 'shifted_2_target', 'shifted_1_imbalance_buy_sell_flag_cumsum', 'shifted_1_rsi_cumsum', \n",
    "            'shifted_2_rsi_cumsum', 'shifted_1_imbalance_cumsum']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f15131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_weights(df):\n",
    "    weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "    ]\n",
    "    match_stock = {}\n",
    "    for idx, w in enumerate(weights):\n",
    "        match_stock[idx] = w\n",
    "    \n",
    "    df['weight_type'] = df['stock_id'].map(match_stock)\n",
    "    return df\n",
    "\n",
    "# cutoff info on seconds_in_bucket\n",
    "def cut_off(seconds_in_bucket):\n",
    "    if seconds_in_bucket <= 300:\n",
    "        return 1\n",
    "    elif seconds_in_bucket <= 480:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "train = match_weights(train)\n",
    "train['cut_off_time'] = train['seconds_in_bucket'].apply(cut_off)\n",
    "\n",
    "stock_pca = pd.read_csv('../data/principal_components.csv')[['stock_id', 'PC1', 'PC2', 'PC3']]\n",
    "\n",
    "train = train.merge(stock_pca, on=['stock_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cbdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third step\n",
    "def global_features(df):\n",
    "    \"\"\"\n",
    "    TODO: size/median_sizes; useful features' ratio;\n",
    "    first_sizes and last_sizes looks wired -> can try to remove them; If they are useful, can consider to \n",
    "    use daily level to create the difference between them;\n",
    "    imbalance_momentum can be update to use multiple windows -> check the difference with the rolling features;\n",
    "    referene_prices over the middle of bid price and ask price;\n",
    "    If 5 mins features are useful, then we can try to replicate the useful features we have before, same to the cutoff;\n",
    "    Whether independent features are better than sum features -> test\n",
    "    \"\"\"\n",
    "    global_stock_id_feats = {\n",
    "        # size related features\n",
    "        \"median_size\": df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"median_bid_size\": df.groupby(\"stock_id\")['bid_size'].median(),\n",
    "        \"median_ask_size\": df.groupby(\"stock_id\")['ask_size'].median(),\n",
    "        \"median_imbalance_size\": df.groupby(\"stock_id\")['imbalance_size'].median(),\n",
    "        \"median_matched_size\": df.groupby(\"stock_id\")['matched_size'].median(),\n",
    "        \"mean_imbalance_buy_sell_flag\": df.groupby(\"stock_id\")['imbalance_buy_sell_flag'].mean(),\n",
    "        \"std_wap\": df.groupby(\"stock_id\")[\"wap\"].std(),\n",
    "        \n",
    "        \"std_size\": df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"max_sizes\": df.groupby('stock_id')['bid_size'].max() + df.groupby('stock_id')['ask_size'].max(),\n",
    "        \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"mean_sizes\": df.groupby('stock_id')['bid_size'].mean() + df.groupby('stock_id')['ask_size'].mean(),\n",
    "        \"ptp_IQR\": df.groupby(\"stock_id\")[\"bid_size\"].quantile(0.75) - df.groupby(\"stock_id\")[\"ask_size\"].quantile(0.25),\n",
    "        \n",
    "        # price related features\n",
    "        \"median_price\": df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        \n",
    "        # 5 mins behavior\n",
    "        \"last5min_medvol\": df[df.seconds_in_bucket>=300].groupby(\"stock_id\")['bid_size'].median() + df[df.seconds_in_bucket>=300].groupby(\"stock_id\")['ask_size'].median(),\n",
    "        \"first5min_medvol\": df[df.seconds_in_bucket<300].groupby(\"stock_id\")['bid_size'].median() + df[df.seconds_in_bucket<300].groupby(\"stock_id\")['ask_size'].median(),\n",
    "        \n",
    "        # cut-off related features\n",
    "        \"cutoff_mean_sizes\": df.groupby(['stock_id', 'cut_off_time'])['bid_size'].mean() + df.groupby(['stock_id', 'cut_off_time'])['ask_size'].mean(),\n",
    "        \"cutoff_imb_ratios\": df.groupby(['stock_id', 'cut_off_time'])['matched_size'].mean() / df.groupby(['stock_id', 'cut_off_time'])['imbalance_size'].mean(),\n",
    "    }\n",
    "\n",
    "    return global_stock_id_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c78969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3be62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step\n",
    "def feature_eng(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60   \n",
    "        \n",
    "    df[\"imbalance\"] = df.eval('imbalance_size * imbalance_buy_sell_flag')\n",
    "    df[\"market_imbalance_buy_sell_flag\"] = (df['bid_size'] > df['ask_size']).astype(int)\n",
    "    df[\"mid_size\"] = df.eval(\"(ask_size + bid_size)/2\")\n",
    "    df['high_volume'] = np.where(2*df['mid_size'] > df['global_median_size'], 1, 0)\n",
    "    df['volume_global_ratio'] = df.eval(\"mid_size/global_median_size\")\n",
    "    df['bid_size_global_ratio'] = df.eval(\"bid_size/global_median_bid_size\")\n",
    "    df['ask_size_global_ratio'] = df.eval(\"ask_size/global_median_ask_size\")\n",
    "    df['imbalance_global_ratio'] = df.eval(\"imbalance_size/global_median_imbalance_size\")\n",
    "    df['matched_global_ratio'] = df.eval(\"matched_size/global_median_matched_size\")\n",
    "    df['bid_ask_volume_diff'] = df.eval(\"ask_size - bid_size\")\n",
    "    df[\"liquidity_imbalance\"] = df['bid_ask_volume_diff']/df[\"mid_size\"]\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_percentage\"] = df[\"price_spread\"]/df[\"mid_price\"]\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / (1+df['matched_size'])\n",
    "    \n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df[\"spread_intensity1\"] = df[\"spread_intensity\"] / df['wap']\n",
    "    df['price_pressure'] = df['imbalance_size'] * df[\"price_spread\"]\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    #供需市场的差额\n",
    "    df['depth_pressure'] = df['bid_ask_volume_diff'] * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df[\"matched_momentum\"] = df.groupby(['stock_id'])['matched_size'].diff(periods=1) / df['imbalance_size']  \n",
    "    \n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\", \"mid_price\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\", \"mid_size\"] \n",
    "    norm_features = ['wap', 'imbalance_buy_sell_flag', 'log_return', 'wap_mid_price_imb']\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\", \"max\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "    \n",
    "    df['near_ratio'] = df['near_price'] / df['reference_price']\n",
    "    df['far_ratio'] = df['far_price'] / df['reference_price']\n",
    "    df['near_size'] = df['near_ratio'] * df['matched_size']\n",
    "    df['far_size'] = df['far_ratio'] * df['matched_size']\n",
    "\n",
    "    # use the triplet method\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "    for c in combinations(sizes, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "    \n",
    "    df['log_wap'] = np.log(df['wap'])\n",
    "    df['auction_direction_alignment'] = (df.groupby(['stock_id', 'date_id'])['wap'].diff() * df['imbalance_buy_sell_flag'] > 0).astype(int)\n",
    "    df['market_direction_alignment'] = (df.groupby(['stock_id', 'date_id'])['wap'].diff() * df['market_imbalance_buy_sell_flag'] > 0).astype(int)\n",
    "    df['log_return'] = df.groupby(['stock_id', 'date_id'])['log_wap'].diff()\n",
    "    \n",
    "    for c in norm_features:\n",
    "            df[f'norm_{c}'] = df.groupby(\"time_id\")[c].transform(lambda x: x - x.mean())\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    rank_df = df.groupby('time_id')[['wap', 'imbalance_buy_sell_flag']].rank(pct=True)\n",
    "    rank_df.columns = [f'{c}_rank' for c in rank_df.columns]\n",
    "    df = pd.concat([df, rank_df], axis=1)\n",
    "        \n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].pct_change(window)\n",
    "            df[f\"wt_{col}_ret_{window}\"] = df.groupby(['weight_type', 'date_id'])[col].pct_change(window)\n",
    "            \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(['stock_id', 'date_id'])[col].diff(window)\n",
    "    \n",
    "    # one hot encoding\n",
    "    df['cut_off_time_2'] = 0\n",
    "    df['cut_off_time_3'] = 0\n",
    "    df.loc[df[\"cut_off_time\"] == 2, \"cut_off_time_2\"] = 1\n",
    "    df.loc[df[\"cut_off_time\"] == 3, \"cut_off_time_3\"] = 1\n",
    "    # df.drop(columns=['cut_off_time'], inplace=True)\n",
    "    df['far_near_diff'] = df['far_price'] - df['near_price']\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df \n",
    "    \n",
    "#     # useful for the last fold\n",
    "#     df['all_prices_mean_std'] = df['all_prices_mean']/(1+df['all_prices_std'])\n",
    "#     df['all_sizes_mean_std'] = df['all_sizes_mean']/(1+df['all_sizes_std'])\n",
    "#     df['all_prices_skew_kurt'] = df['all_prices_skew']/(1+df['all_prices_kurt'])\n",
    "#     df['all_sizes_skew_kurt'] = df['all_sizes_skew']/(1+df['all_sizes_kurt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24e26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\"]]\n",
    "    df = df[cols]\n",
    "\n",
    "    global_stock_id_feats = global_features(df)\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        if 'cut_off' not in key:\n",
    "            df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "        else:\n",
    "            df[f\"global_{key}\"] = df[\"stock_id\", 'cut_off_time'].map(value.to_dict())\n",
    "            \n",
    "    df = feature_eng(df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Select and return the generated features\n",
    "    feature_name = [i for i in df.columns if\n",
    "                    i not in [\"row_id\", \"target\", \"time_id\", \"date_id\", \"index_return\", \"stock_return\", \"target_norm\",\n",
    "                              \"currently_scored\"]]\n",
    "\n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02693bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values(by=['stock_id', 'date_id', 'seconds_in_bucket'], ascending=True, inplace=True)\n",
    "train = train[:100000]\n",
    "\n",
    "time_id = train.time_id.values\n",
    "date_id = train.date_id.values\n",
    "time_index = time_id % 55\n",
    "stock_id = train.stock_id.values\n",
    "y = train['target']\n",
    "\n",
    "train = generate_all_features(train)\n",
    "train['target'] = y\n",
    "\n",
    "ntime = 55\n",
    "ndate = len(np.unique(date_id))\n",
    "nstock = 200\n",
    "nfeatures = train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c50861",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = ['upper_matched_size', 'lower_matched_size', 'upper_wap', 'lower_wap', 'upper_reference_price',\\\n",
    "                  'lower_reference_price']\n",
    "\n",
    "X = np.zeros((ndate, ntime, nstock, nfeatures+len(extra_features))) * np.nan\n",
    "feature_dict = {}\n",
    "for i, c in enumerate(train.columns.tolist()):\n",
    "    feature_dict[c] = i\n",
    "    \n",
    "X[date_id, time_index, stock_id, :nfeatures] = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9733009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_id'] = time_id\n",
    "train['date_id'] = date_id\n",
    "train_l = []\n",
    "\n",
    "for (t), frame in train.groupby(\"time_id\"):\n",
    "    stock = frame.stock_id.values\n",
    "    date = frame.date_id.values[0]\n",
    "    time = t % 55\n",
    "    # generate_features(X, date, time_idx, stock_idx, feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4178b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(X, date, time, stock, feature_dict):\n",
    "    res = pd.DataFrame(columns=list(feature_dict.keys()))\n",
    "    res[list(feature_dict.keys())] = X[date, time, :, :nfeatures]\n",
    "    for i, col in enumerate(['matched_size', 'wap', 'reference_price']):\n",
    "        res[f'cum_{col}_std'] = np.std(X[date, :time + 1, :, feature_dict[col]], axis=0)\n",
    "        X[date, time, :, nfeatures+2*i] = X[date, time, :, feature_dict[col]] + res[f'cum_{col}_std']\n",
    "        X[date, time, :, nfeatures+2*i+1] = X[date, time, :, feature_dict[col]] - res[f'cum_{col}_std']\n",
    "        if (time - 7 + 1 < 0):\n",
    "            res[f'rolling_upper_{col}_max'] = np.nan\n",
    "            res[f'rolling_upper_{col}_min'] = np.nan\n",
    "            res[f'rti_{col}'] = np.nan\n",
    "        else:\n",
    "            res[f'rolling_upper_{col}_max'] = np.max(X[date, time - 7 + 1:time + 1, :, nfeatures+2*i], axis=0)\n",
    "            res[f'rolling_upper_{col}_min'] = np.min(X[date, time - 7 + 1:time + 1, :, nfeatures+2*i+1], axis=0)\n",
    "            res[f'rti_{col}'] = (res[col] - res[f'rolling_upper_{col}_min'])/(res[f'rolling_upper_{col}_max'] - res[f'rolling_upper_{col}_min'])\n",
    "    res.drop(columns=['cum_wap_std'], inplace=True)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=list(feature_dict.keys()))\n",
    "res[list(feature_dict.keys())] = X[date, time, :, :nfeatures]\n",
    "first_features = ['matched_size', 'imbalance_size', 'imbalance_buy_sell_flag', 'ask_size', 'bid_size']\n",
    "rank_features = ['imbalance_buy_sell_flag', 'wap', 'wap_mid_price_imb',\n",
    "                 'volume_global_ratio', 'imbalance_global_ratio', 'matched_global_ratio', 'bid_size_global_ratio',\n",
    "                 'ask_size_global_ratio', 'market_imbalance_buy_sell_flag']\n",
    "\n",
    "for f in first_features:\n",
    "    c_first = X[date, 0, :, feature_dict[f]]\n",
    "    c_curr = X[date, time, :, feature_dict[f]]\n",
    "    try:\n",
    "        first_ratio = c_curr / c_first\n",
    "    except:\n",
    "        first_ratio = np.nan\n",
    "    if(f\"{f}_first\" in features):\n",
    "        res[f\"{f}_first\"] = c_first\n",
    "    if (f\"{f}_first_ratio\" in features):\n",
    "        res[f\"{f}_first_ratio\"] = first_ratio\n",
    "\n",
    "## Cumulative Features\n",
    "cumulative_features = ['imbalance_buy_sell_flag','rsi']\n",
    "for f in cumulative_features:\n",
    "    cumsum = np.sum(np.nan_to_num(X[date, :time + 1, :, feature_dict[f]]), axis=0)\n",
    "    try:\n",
    "        cummean = cumsum / X[date, time, :, feature_dict['seconds_in_bucket']]\n",
    "    except:\n",
    "        cummean = np.nan\n",
    "    if(f'{f}_cumsum' in features):\n",
    "        res[f'{f}_cumsum'] = cumsum\n",
    "    if(f'{f}_cummean' in features):\n",
    "        res[f'{f}_cummean'] = cummean\n",
    "        \n",
    "for f in rank_features:\n",
    "    if(f\"{f}_rank\" in features):\n",
    "        c_curr = X[date, time, :, feature_dict[f]]\n",
    "        res[f\"{f}_rank\"] = pd.Series(c_curr).rank(pct=True).values\n",
    "        \n",
    "## Additional Rank Features\n",
    "res[f\"imbalance_buy_sell_flag_cumsum_rank\"] = res['imbalance_buy_sell_flag_cumsum'].rank(pct=True).values\n",
    "\n",
    "## Percent Change Features\n",
    "for f in ['matched_size', 'imbalance_size', 'reference_price', 'wap', 'ask_price', 'bid_price', 'ask_size','bid_size']:\n",
    "    for window in [1, 2, 3, 6, 10]:\n",
    "        if(f\"{f}_ret_{window}\" in features):\n",
    "            try:\n",
    "                pct_change = (X[date, time, :, feature_dict[f]] / X[date, time - window, :, feature_dict[f]] - 1)\n",
    "            except:\n",
    "                pct_change = np.nan\n",
    "            res[f\"{f}_ret_{window}\"] = pct_change\n",
    "            if (time - window < 0):\n",
    "                res[f\"{f}_ret_{window}\"] = np.nan\n",
    "        \n",
    "        # last 10 sec wap and the ratio between wap and the mean of previous wap in the same weight type\n",
    "        if f == 'wap' and window==1:\n",
    "            group_keys = X[date, time-window, :, feature_dict['weight_type']]\n",
    "\n",
    "            mean_values = np.zeros_like(group_keys, dtype=float)  # Initialize an array to store mean values\n",
    "\n",
    "            unique_keys = np.unique(group_keys)  # Get unique group keys\n",
    "\n",
    "            for key in unique_keys:\n",
    "                mask = (group_keys == key)  # Create a mask to filter values based on the current key\n",
    "                mean_val = np.mean(X[date, time-window, :, feature_dict[f]][mask])  # Calculate mean of column 4 based on the mask\n",
    "                mean_values[mask] = mean_val  # Assign mean value to corresponding positions in the array\n",
    "            res['previous_wap'] = X[date, time - window, :, feature_dict[f]]\n",
    "            res['previous_wt_wap_mean'] = mean_values\n",
    "            res['wap_percent_ind'] = X[date, time, :, feature_dict[f]]/mean_values\n",
    "        \n",
    "        # last 10 sec median match size and im size for each weight type and the ratio between im size and match size\n",
    "        if window==1 and (f=='imbalance_size' or f=='matched_size'):\n",
    "            group_keys = X[date, time-window, :, feature_dict['weight_type']]\n",
    "\n",
    "            median_values = np.zeros_like(group_keys, dtype=float)  # Initialize an array to store mean values\n",
    "\n",
    "            unique_keys = np.unique(group_keys)  # Get unique group keys\n",
    "\n",
    "            for key in unique_keys:\n",
    "                mask = (group_keys == key)  # Create a mask to filter values based on the current key\n",
    "                median_val = np.median(X[date, time-window, :, feature_dict[f]][mask])  # Calculate mean of column 4 based on the mask\n",
    "                median_values[mask] = median_val  # Assign mean value to corresponding positions in the array\n",
    "            res[f'previous_wt_{f}_median'] = median_values\n",
    "res['previous_imbalance_ratio_percent_ind'] =  X[date, time, :, feature_dict['imbalance_size']] / (1+X[date, time, :, feature_dict['matched_size']])/ res['previous_wt_imbalance_size_median'] / (1+res['previous_wt_matched_size_median'])            \n",
    "res.drop(columns=['previous_wt_imbalance_size_median', 'previous_wt_matched_size_median'], inplace=True)     \n",
    "            \n",
    "## Rolling Means\n",
    "for f in ['wap', 'imbalance_buy_sell_flag', 'imbalance_size', 'matched_size', 'norm_wap','auction_direction_alignment','rsi']:\n",
    "    if(f\"{f}_rolling_mean_{window}\" in features):\n",
    "        for window in [3, 6, 10]:\n",
    "            mean = np.mean(X[date, time - window + 1:time + 1, :, feature_dict[f]], axis=0)\n",
    "            res[f\"{f}_rolling_mean_{window}\"] = mean\n",
    "            if (time - window + 1 < 0):\n",
    "                res[f\"{f}_rolling_mean_{window}\"] = np.nan\n",
    "\n",
    "## Rolling Standard Deviations\n",
    "for f in ['wap', 'imbalance_buy_sell_flag', 'imbalance_size', 'matched_size', 'norm_wap']:\n",
    "    if(f\"{f}_rolling_std_{window}\" in features):\n",
    "        for window in [3, 6, 10]:\n",
    "            std = np.std(X[date, time - window + 1:time + 1, :, feature_dict[f]], axis=0)\n",
    "            res[f\"{f}_rolling_std_{window}\"] = std\n",
    "            if (time - window + 1 < 0):\n",
    "                res[f\"{f}_rolling_std_{window}\"] = np.nan\n",
    "                \n",
    "## EMA\n",
    "alpha = 0.285\n",
    "beta = 1 - alpha\n",
    "for f in ['matched_size', 'wap', 'imbalance_size','norm_wap','reference_price']:\n",
    "    if(f\"{f}_ema\" in features):\n",
    "        ema = X[date, time, :, feature_dict[f]]*alpha + \\\n",
    "              X[date, time-1, :, feature_dict[f]]*alpha*beta + \\\n",
    "              X[date, time-2, :, feature_dict[f]]*alpha*beta**2 + \\\n",
    "              X[date, time-3, :, feature_dict[f]]*alpha*beta**3 + \\\n",
    "              X[date, time-4, :, feature_dict[f]]*alpha*beta**4 + \\\n",
    "              X[date, time-5, :, feature_dict[f]]*alpha*beta**5 + \\\n",
    "              X[date, time-6, :, feature_dict[f]]*alpha*beta**6\n",
    "        res[f\"{f}_ema\"] = ema\n",
    "        if (time < 6):\n",
    "            res[f\"{f}_ema\"] = np.nan\n",
    "            \n",
    "for f in ['wap_mid_price_imb', 'reference_price_wap_imb', 'norm_wap', 'imbalance_buy_sell_flag', 'wap_rank','imbalance_buy_sell_flag_rank','norm_log_return']:\n",
    "    for window in [1, 2, 3, 4, 5, 6]:\n",
    "        if(f\"{f}_shift_{window}\" in features):\n",
    "            lag = X[date, time - window, :, feature_dict[f]]\n",
    "            res[f\"{f}_shift_{window}\"] = lag\n",
    "            if (time - window < 0):\n",
    "                res[f\"{f}_shift_{window}\"] = np.nan\n",
    "\n",
    "shift_features = ['imbalance_size', 'imbalance_buy_sell_flag', 'wap_rank', 'imbalance_buy_sell_flag_rank','reference_price_wap_imb', 'target']\n",
    "for shift_idx in [1, 2]:\n",
    "    for f in shift_features:\n",
    "        if(f\"shifted_{shift_idx}_{f}\" in features):\n",
    "            shift = X[date - shift_idx, time, :, feature_dict[f]].copy()\n",
    "            res[f\"shifted_{shift_idx}_{f}\"] = shift\n",
    "            if (date - shift_idx < 0):\n",
    "                res[f\"shifted_{shift_idx}_{f}\"] = np.nan\n",
    "\n",
    "# Handling edge case cumsum features\n",
    "for f in ['imbalance_buy_sell_flag','rsi','imbalance']:\n",
    "    for shift_idx in [1, 2]:\n",
    "        if(f\"shifted_{shift_idx}_{f}_cumsum\" in features):\n",
    "            cumsum = np.sum(np.nan_to_num(X[date - shift_idx, :time + 1, :, feature_dict[f]]), axis=0)\n",
    "            res[f\"shifted_{shift_idx}_{f}_cumsum\"] = cumsum\n",
    "            if (date - shift_idx < 0):\n",
    "                res[f\"shifted_{shift_idx}_{f}_cumsum\"] = np.nan                \n",
    "        \n",
    "# adjusted rti\n",
    "for i, col in enumerate(['matched_size', 'wap', 'reference_price']):\n",
    "    res[f'cum_{col}_std'] = np.std(X[date, :time + 1, :, feature_dict[col]], axis=0)\n",
    "    X[date, time, :, nfeatures+2*i] = X[date, time, :, feature_dict[col]] + res[f'cum_{col}_std']\n",
    "    X[date, time, :, nfeatures+2*i+1] = X[date, time, :, feature_dict[col]] - res[f'cum_{col}_std']\n",
    "    if (time - 7 + 1 < 0):\n",
    "        res[f'rolling_upper_{col}_max'] = np.nan\n",
    "        res[f'rolling_upper_{col}_min'] = np.nan\n",
    "        res[f'rti_{col}'] = np.nan\n",
    "    else:\n",
    "        res[f'rolling_upper_{col}_max'] = np.max(X[date, time - 7 + 1:time + 1, :, nfeatures+2*i], axis=0)\n",
    "        res[f'rolling_upper_{col}_min'] = np.min(X[date, time - 7 + 1:time + 1, :, nfeatures+2*i+1], axis=0)\n",
    "        res[f'rti_{col}'] = (res[col] - res[f'rolling_upper_{col}_min'])/(res[f'rolling_upper_{col}_max'] - res[f'rolling_upper_{col}_min'])\n",
    "res.drop(columns=['cum_wap_std'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1mins level wap and industries related features -> try to lower the difficulties on predicting the target\n",
    "df['previous_target'] = df['1min_wap_ratio'] - df['1min_iwap_ratio']\n",
    "df[f'cum_previous_target_mean'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().mean().reset_index(level=[0,1], drop=True)\n",
    "df[f'cum_previous_target_std'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().std().reset_index(level=[0,1], drop=True)\n",
    "df[f'cum_previous_target_skew'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().skew().reset_index(level=[0, 1], drop=True)\n",
    "df[f'cum_previous_target_kurtosis'] = df.groupby(['stock_id', 'date_id'])['previous_target'].expanding().kurt().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# cumulative min, max and min max standard of the reference price, matched_size, imbalance_size\n",
    "for col in ['reference_price', 'matched_size', 'imbalance_size']:\n",
    "    df[f'cum_{col}_min'] = df.groupby(['stock_id', 'date_id'])[col].expanding().min().reset_index(level=[0,1], drop=True)\n",
    "    df[f'cum_{col}_max'] = df.groupby(['stock_id', 'date_id'])[col].expanding().max().reset_index(level=[0,1], drop=True)\n",
    "    df[f'cum_{col}_max_min'] = (df[col] - df[f'cum_{col}_min'])/(1+df[f'cum_{col}_max'] - df[f'cum_{col}_min'])\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "### Heavy Ops ###\n",
    "\n",
    "\n",
    "        \n",
    "    # drop at the end\n",
    "    # df.drop(columns=['cum_reference_price_min', 'cum_imbalance_size_min'], inplace=True)\n",
    "        \n",
    "    # pct_change for reference_price and wap and get the skew and kurt\n",
    "    for col in ['reference_price', 'wap',]:\n",
    "        df[f'{col}_pct_change'] = df.groupby(['stock_id', 'date_id'])[col].pct_change()\n",
    "        df[f'cum_{col}_skew'] = df.groupby(['stock_id', 'date_id'])[f'{col}_pct_change'].expanding().skew().reset_index(level=[0, 1], drop=True)\n",
    "        df[f'cum_{col}_kurtosis'] = df.groupby(['stock_id', 'date_id'])[f'{col}_pct_change'].expanding().kurt().reset_index(level=[0, 1], drop=True)\n",
    "        \n",
    "        # vol features\n",
    "        df[f'abs_return'] = df[f'{col}_pct_change'].abs()\n",
    "        df[f'{col}_illiq'] = (df['abs_return'] / df['matched_size']).rolling(10).mean()\n",
    "        \n",
    "        df['neg_retrun_flag'] = np.where(df[f'{col}_pct_change']<0, 1, 0)\n",
    "        df['neg_retrun_flag'] = df['neg_retrun_flag'] * df['abs_return']\n",
    "        df[f'{col}_negative_returns_illiq'] = df['neg_retrun_flag'].rolling(10).sum() / (1+(df['imbalance_size']/df['matched_size']).rolling(10).sum())\n",
    "        \n",
    "    # shrink flag\n",
    "    for col in ['matched_size', 'imbalance_size',]:\n",
    "        for window in [7, 14]:\n",
    "            df[f'{col}_{window}_shrink_vol'] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=window).std().reset_index(level=[0,1], drop=True)\n",
    "            df[f'{col}_{window}_shrink_vol'] = np.where(df[col]<2*df[f'{col}_{window}_shrink_vol'], 1, 0)\n",
    "    \n",
    "    df.drop(columns=['previous_im_size', 'previous_m_size', 'previous_wt_im_median', 'previous_wt_m_median', \\\n",
    "                    'upper_matched_size', 'lower_matched_size', 'upper_wap', 'lower_wap', \\\n",
    "                    'upper_reference_price', 'lower_reference_price', 'buy_site_flag', 'sell_site_flag', \\\n",
    "                    'buy_flag_diff', 'sell_flag_diff', 'buy_site_diff_sum', 'sell_site_diff_sum', \\\n",
    "                    'buy_flag', 'sell_flag', 'buy_time', 'sell_time', 'buy_time_start', 'buy_time_end', \\\n",
    "                    'sell_time_start', 'sell_time_end', 'buy_time_diff', 'sell_time_diff', \\\n",
    "                    'cum_wap_skew', 'reference_price_diff', \\\n",
    "                    'cum_wap_std', 'cum_reference_price_min', 'cum_imbalance_size_min', 'index_wap', 'reference_price_pct_change',\\\n",
    "                    'abs_return', 'neg_retrun_flag', 'previous_1_min',\\\n",
    "                    ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc530d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=list(feature_dict.keys()))\n",
    "res[list(feature_dict.keys())] = X[date, time, :, :nfeatures]\n",
    "first_features = ['matched_size', 'imbalance_size', 'imbalance_buy_sell_flag', 'ask_size', 'bid_size']\n",
    "rank_features = ['imbalance_buy_sell_flag', 'wap', 'wap_mid_price_imb',\n",
    "                 'volume_global_ratio', 'imbalance_global_ratio', 'matched_global_ratio', 'bid_size_global_ratio',\n",
    "                 'ask_size_global_ratio', 'market_imbalance_buy_sell_flag']\n",
    "\n",
    "for f in ['matched_size', 'imbalance_size', 'reference_price', 'wap', 'ask_price', 'bid_price', 'ask_size','bid_size']:\n",
    "    for window in [1, 2, 3, 6, 10]:\n",
    "        if(f\"{f}_ret_{window}\" in features):\n",
    "            try:\n",
    "                pct_change = (X[date, time, :, feature_dict[f]] / X[date, time - window, :, feature_dict[f]] - 1)\n",
    "            except:\n",
    "                pct_change = np.nan\n",
    "            res[f\"{f}_ret_{window}\"] = pct_change\n",
    "            if (time - window < 0):\n",
    "                res[f\"{f}_ret_{window}\"] = np.nan\n",
    "        \n",
    "        # last 10 sec wap and the ratio between wap and the mean of previous wap in the same weight type\n",
    "        if f == 'wap' and window==1:\n",
    "            group_keys = X[date, time-window, :, feature_dict['weight_type']]\n",
    "\n",
    "            mean_values = np.zeros_like(group_keys, dtype=float)  # Initialize an array to store mean values\n",
    "\n",
    "            unique_keys = np.unique(group_keys)  # Get unique group keys\n",
    "\n",
    "            for key in unique_keys:\n",
    "                mask = (group_keys == key)  # Create a mask to filter values based on the current key\n",
    "                mean_val = np.mean(X[date, time-window, :, feature_dict[f]][mask])  # Calculate mean of column 4 based on the mask\n",
    "                mean_values[mask] = mean_val  # Assign mean value to corresponding positions in the array\n",
    "            res['previous_wap'] = X[date, time - window, :, feature_dict[f]]\n",
    "            res['previous_wt_wap_mean'] = mean_values\n",
    "            res['wap_percent_ind'] = X[date, time, :, feature_dict[f]]/mean_values\n",
    "        \n",
    "        # last 10 sec median match size and im size for each weight type and the ratio between im size and match size\n",
    "        if window==1 and (f=='imbalance_size' or f=='matched_size'):\n",
    "            group_keys = X[date, time-window, :, feature_dict['weight_type']]\n",
    "\n",
    "            median_values = np.zeros_like(group_keys, dtype=float)  # Initialize an array to store mean values\n",
    "\n",
    "            unique_keys = np.unique(group_keys)  # Get unique group keys\n",
    "\n",
    "            for key in unique_keys:\n",
    "                mask = (group_keys == key)  # Create a mask to filter values based on the current key\n",
    "                median_val = np.median(X[date, time-window, :, feature_dict[f]][mask])  # Calculate mean of column 4 based on the mask\n",
    "                median_values[mask] = median_val  # Assign mean value to corresponding positions in the array\n",
    "            res[f'previous_wt_{f}_median'] = median_values\n",
    "res['previous_imbalance_ratio_percent_ind'] =  X[date, time, :, feature_dict['imbalance_size']] / (1+X[date, time, :, feature_dict['matched_size']])/ res['previous_wt_imbalance_size_median'] / (1+res['previous_wt_matched_size_median'])            \n",
    "res.drop(columns=['previous_wt_imbalance_size_median', 'previous_wt_matched_size_median'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a44279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous_imbalance_ratio_percent_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.754543e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.478304e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.143392e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     previous_imbalance_ratio_percent_ind\n",
       "0                            5.754543e-16\n",
       "1                            2.478304e-15\n",
       "2                            8.143392e-15\n",
       "3                                     NaN\n",
       "4                                     NaN\n",
       "..                                    ...\n",
       "195                                   NaN\n",
       "196                                   NaN\n",
       "197                                   NaN\n",
       "198                                   NaN\n",
       "199                                   NaN\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[['previous_imbalance_ratio_percent_ind']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca157f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
